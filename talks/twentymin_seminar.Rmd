---
title: "False Discovery Rates, A New Deal"
author: Matthew Stephens
date: June 12, 2015
output: ioslides_presentation
---


```{r, include=FALSE}
require("qvalue")
require("ashr")
require("ggplot2")
require("knitr")
```

```{r setup, include=FALSE}
  #set global chunk options
  opts_chunk$set(cache=FALSE,autodep=TRUE,warning=FALSE)
  dep_auto()
```

## How can we make more progress together?

- Abstract our problems
- Develop benchmarks
- Build generic tools


## The power of generic tools

- Linear regression
- k-means clustering
- Principal Components Analysis
- qvalue for FDR

Goal: generic tools for shrinkage estimation


## The Canonical Genomics Experiment 

- Measure lots of things, with error

- Get estimates of effects $\beta_j$ ($\hat\beta_j$) and their standard errors $s_j$

- Turn these into Z-scores, $z_j = \hat\beta_j/s_j$

- Turn these into $p$ values, $p_j$

- Apply `qvalue` 
to estimate FDR at any given threshold.

$$\text{FDR}(\gamma)=\Pr(\beta_j = 0 |  p_j< \gamma).$$


- ...


```{r, echo=FALSE}
  #simple simulated example
  ncz = 100 # number of bins in z score histograms
  set.seed(111)
  hh.betahat = rnorm(1000,0,2)
  hh.sebetahat = 1
  hh.zscore = hh.betahat/hh.sebetahat
  hh.pval = pchisq(hh.zscore^2,df=1,lower.tail=F)
  hh.ash = ash(hh.betahat,hh.sebetahat, method="fdr")
  hh.q = qvalue(hh.pval)
```



```{r, echo=FALSE}
  plot_FDReg_hist = function(hh.pval,pi0,nc=40,nullcol="blue",altcol="cyan",type=4,title="Distribution of p values",...){
    hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main=title,nclass=nc,col=altcol,...)
    if(type>1){
      abline(h=pi0,col=nullcol,lwd=2)
    
      hh.hist$density=rep(pi0,length(hh.hist$density))  
      #hh.hist$counts=rep(hh.q$pi0*length(hh.pval)/nc,length(hh.hist$counts)) 
      plot(hh.hist,add=TRUE,col=nullcol,freq=FALSE)
    }
    if(type>2){
    abline(v=0.1,lwd=2,col=2)
    }
    if(type>3){
      text(0.05,1.2,labels="A",col=2,cex=1.2)  
      text(0.05,0.4,labels="B",col=2,cex=1.2)  
      text(0.6,3,labels=paste0("FDR = B/(A+B) =  ",round(pi0*0.1*length(hh.pval)/sum(hh.pval<0.1),2)),cex=1.2)
    }
  }
```


## Example: FDR estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  plot_FDReg_hist(hh.pval,hh.q$pi0,type=1)                 
```

## Example: FDR estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  plot_FDReg_hist(hh.pval,hh.q$pi0,type=2)                 
```

## Example: FDR estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  plot_FDReg_hist(hh.pval,hh.q$pi0,type=3)                 
```

## Example: FDR estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  plot_FDReg_hist(hh.pval,hh.q$pi0,type=4)                 
```

## The outsize impact of generic procedures 

- The original paper introducing FDR (Benjamini and Hochberg, 1995) has
been cited 27,331 times (June 2015) according to Google Scholar.

- That is three times a day for the last 20 years!

- More than ten times a day for the last year!

## Problem 1: The Zero Assumption (ZA)

- The standard `qvalue` 
approach assumes that all the $p$ values near 1 are null.

- Analogously, one can assume that all Z scores near 0 are null. Efron refers to this as the ``Zero Assumption".

- Seems initially natural.


```{r, echo=FALSE, include=FALSE}
require(fdrtool)
  hh.fdrtool = fdrtool(hh.pval,statistic="pvalue",plot=FALSE)
require(locfdr)
  hh.locfdr = locfdr(hh.zscore,nulltype=0,plot=0)
require(mixfdr)
  hh.mixfdr = mixFdr(hh.zscore,noiseSD=1,theonull=TRUE,plot=FALSE)
```

## Implied distribution of $p$ values under $H_1$

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  plot_FDReg_hist(hh.pval,hh.q$pi0,type=4)                 
```


## Implied distribution of Z scores under alternative

```{r, echo=FALSE,fig.height=4,fig.cap=""}
#plot a histogram of z scores, highlighting the alternative distribution
#of z scores that is implied by localfdr values lfdr.
  nullalthist = function(z,lfdr,nullcol="blue",altcol="cyan",...){
    h=hist(z, freq=FALSE,col=nullcol,nclass=ncz,...)
    avlfdr = unlist(lapply(split(lfdr,cut(z,h$breaks),drop=FALSE),mean))
    h$density = (1-avlfdr) * h$density
    plot(h,add=TRUE,col=altcol,freq=FALSE)
  }
   
  nullalthist(hh.zscore,hh.fdrtool$lfdr)  
```



## FDR problem 2: different measurement precision

- In some cases the measurement precisions differ among units

- Eg effect sizes of rare SNPs have larger standard error than those of common SNPs

- Eg Expression levels of low-expressed genes have less precision than high-expressed genes

- If some effects are measured less precisely than others, those tests ``lack power"
and dilute signal, increasing FDR


```{r, echo=FALSE}
#install q value package
#source("http://bioconductor.org/biocLite.R")
#biocLite("qvalue")
library("qvalue")
library("lattice") #library for some of the plots

#set up some data with mixture of values of s
set.seed(100)
s.good = 0.5
s.poor = 10
J.good= 500
J.poor = 500
J=J.good+J.poor
beta = c(rnorm(J,0,1))
s = c(rep(s.good,J.good),rep(s.poor,J.poor))
betahat = beta + rnorm(J,0,s)
#compute the usual zscore and corresponding p value
zscore = betahat/s
pval = pchisq(zscore^2,df=1,lower.tail=F)
```

## FDR problem 2: different measurement precision

- Simulation: effects $\beta_j \sim N(0,1)$
- 500 "good" observations with low standard error ($s_j=0.5$)
- 500 "poor" observations with very high standard error ($s_j=10$)

## FDR problem 2: different measurement precision

```{r, echo=FALSE,fig.height=4,fig.cap=""}
plot_FDReg_hist(pval[1:500],0,title="distribution of GOOD p values",type=1,ylim=c(0,15))
```

## FDR problem 2: different measurement precision

```{r, echo=FALSE,fig.height=4,fig.cap=""}
plot_FDReg_hist(pval[501:1000],0,title="distribution of POOR p values",type=1,ylim=c(0,15))
```

## FDR problem 2: different measurement precision

```{r, echo=FALSE,fig.height=4,fig.cap=""}
plot_FDReg_hist(pval[1:1000],0,title="distribution of ALL p values",type=1,ylim=c(0,15))
```
```{r, include=FALSE}
qq.all = qvalue(pval)
qq.good = qvalue(pval[1:500])
```

## FDR problem 2: different measurement precision

```{r, echo=FALSE,fig.height=4,fig.cap=""}
plot_FDReg_hist(pval[1:500],qq.good$pi0,title="GOOD p values",type=4,ylim=c(0,15))
```

## FDR problem 2: different measurement precision

```{r, echo=FALSE,fig.height=4,fig.cap=""}
plot_FDReg_hist(pval[1:1000],qq.all$pi0,title="ALL p values",type=4,ylim=c(0,15))
```



## Problems: Summary

- The ZA, which implies actual effects have a (probably unrealistic) bimodal distribution; causes overestimate of $\pi_0$, losing power.

- By focussing on $p$ values, low-precision measurements can dilute high-precision measurements.

(Both tend to lead to conservative behaviour: overestimating FDR)


## Can we do better?

Two Ideas:

- Use two numbers ($\hat\beta_j,s_j$) instead of one number ($p_j$ or $z_j$).

- Introduce "modelling assumption": distribution of $\beta_j$ is unimodal about 0.


## FDR via Empirical Bayes

- Following previous work (e.g. Newton, Efron, Muralidharan) we take an empirical Bayes approach to FDR.

- Eg Efron assumes that the $Z$ scores come from a mixture of null, and alternative:
$$Z_j \sim f_Z(.) = \pi_0 N(.;0,1) + (1-\pi_0) f_1(.)$$
where $f_1, \pi_0$ are to be estimated from the data.

- Various semi-parametric approaches taken to estimating $f_1$. For example,
Efron uses Poisson regression; Muralidharan uses mixture of normal distributions.

- Once $f_1$ and $\pi_0$ estimated, FDR calculations are straightforward.

## FDR: A New Deal

- Instead of modelling $Z$ scores, model the effects $\beta$,
$$\beta_j \sim \pi_0 \delta_0(.) + (1-\pi_0) g(.)$$

- Constrain $g$ to be unimodal about 0; estimate $g$ from data.

- *Incorporate precision* of each observation $\hat\beta$ into the likelihood:
$$\hat\beta_j | s_j \sim N(\beta_j, s_j)$$

(cf Bayes inference from summary statistics; V.Johnson, J.Wakefield)

## FDR - A New Deal

- A convenient way to model $g$: mixture of 0-centered
normal distributions: 
$$g(\beta; \pi) = \sum_{k=1}^K \pi_k N(\beta; 0, \sigma^2_k)$$

- Estimating $g$ comes down to estimating $\pi$. Joint estimation of $\pi_0,\pi$ easy by maximum likelihood (EM algorithm).

- By allowing $K$ large, and $\sigma_k$ to span a dense grid of values,
we get a flexible unimodal symmetric distribution.

- Can approximate, arbitrarily closely, any scale mixture of normals.
Includes almost all priors used for sparse regression problems (spike-and-slab, double exponential/Laplace/Bayesian Lasso, horseshoe). 

## FDR - A New Deal

- Alternatively, a mixture of uniforms, with 0 as one end-point of the range,
provides still more flexibility, and in particular allows for asymmetry. 

- If allow a very large number of uniforms this provides the non-parametric mle for $g$; cf Grenander 1953; Cordy + Thomas 1997.


## Illustration: $g$ a mixture of 0-centered normals

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, dnorm(x,0,1),type="l",ylim=c(0,2),ylab="density")
lines(x, dnorm(x,0,0.1))
lines(x, dnorm(x,0,0.5))
```

## Illustration: $g$ a mixture of 0-centered normals

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, 0.5*dnorm(x,0,1)+0.5*dnorm(x,0,0.1),type="l",ylim=c(0,2),ylab="density")
```


## Illustration: $g$ a mixture of 0-anchored uniforms

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, dunif(x,0,1),type="s",ylim=c(0,5),ylab="density")
lines(x, dunif(x,0,0.2),type="s")
lines(x, dunif(x,0,0.5),type="s")
lines(x, dunif(x,-0.3,0),type="s",col=2)
lines(x, dunif(x,-0.4,0),type="s",col=2)
lines(x, dunif(x,-2,0),type="s",col=2)
```

## Illustration: $g$ a mixture of 0-anchored uniforms

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, 0.1*dunif(x,0,3)+ 0.3*dunif(x,0,0.2) + 0.2*dunif(x,0,0.5) + 0.1* dunif(x,-0.3,0) + 0.1*dunif(x,-0.3,0)+0.2*dunif(x,-0.4,0),type="s",ylim=c(0,2),ylab="density")
```

## Adaptive Shrinkage

- This approach actually provides a full posterior distribution for each $\beta_j$.

- So easy to obtain point estimates and credible intervals.

- Because $g(\beta)$ is unimodal, the point estimates (and CIs) will tend to be ``shrunk" towards the overall mean (0).

- Because $g(\beta)$ is estimated from the data, the amount
of shrinkage is adaptive to signal in the data. And because of the role of $s_j$, the amount of shrinkage adapts to the information on each observation.

- So we call the approach ``Adaptive Shrinkage" (ASH).



## Recall Problem 1: distribution of alternative Z values multimodal
```{r, echo=FALSE,fig.height=4,fig.cap=""}
nullalthist(hh.zscore,hh.fdrtool$lfdr,main="qvalue")  
```

## Problem Fixed: distribution of alternative Z values unimodal
```{r, echo=FALSE,fig.height=4,fig.cap=""}
nullalthist(hh.zscore,hh.ash$lfdr,main="ash")
```


## Recall Problem 2: poor measurements dilute good measurments

```{r, include=FALSE}
  beta.ash.good = ash(betahat[1:500],s[1:500],method="fdr")
  beta.ash.all = ash(betahat,s,method="fdr")
  print(c(sum(beta.ash.good$qvalue<0.05),sum(beta.ash.all$qvalue<0.05)))
```

Number of findings at (estimated) FDR=0.05:
```{r, echo=FALSE}
  res.tab = data.frame(qvalue = c(sum(qq.all$qvalues<0.05),sum(qq.good$qvalues<0.05)),
                       ash =c(sum(beta.ash.all$qvalue<0.05),sum(beta.ash.good$qvalue<0.05)))
  row.names(res.tab)=c("ALL","GOOD")
  kable(res.tab,row.names=TRUE,format="pandoc")
```

## Some more extensive simulations

![Image](../paper/figures/scenarios_density.pdf)

$$\beta_j \sim \pi_0 \delta_0(.) + (1-\pi_0) g(.)$$

Goals: Estimate $\pi_0$? Estimate $g$?

## Estimation of $\pi_0$

![Image](../paper/figures/pi0_est.pdf)

All methods are, generally, "conservative" (deliberately).

## Estimation of $g$

- Estimating $g$ is a “deconvolution problem”.
- Notoriously difficult in general. 
- Efron emphasises the difficulties of implementing a stable general algorithm:

<div class="red">
“the effort foundered on practical difficulties involving the perils of deconvolution... Maybe I am trying to be overly nonparametric... but it is hard to imagine a generally satisfactory parametric formulation...”
</div>


## Unimodal assumption stabilizes estimation of $g$

![Image](../paper/figures/egcdf.pdf)


## A new problem: an embarrassment of riches

- If the null is mostly false, this approach can  provide unsettling results

- The FDR can be small for all observations, even those with $p \approx 1$!

- In the illustrative example, the maximum $q$ value is `r round(max(hh.ash$qvalue),2)`


## Perhaps we didn't really understand the question?

- Problem arises only if we insist on asking question "is $\beta_j=0$?"

- Given enough signal, we become convinced that very few of the $\beta_j=0$

- But for some $\beta_j$ we still may have little information about actual value

- Suggests a change of focus: ask for which $\beta_j$ are we confident about the sign (cf Gelman et al, 2012; Tukey 1961).


## Generic procedure: summary

- Inputs: $(\hat{\beta}_1,s_1), \dots, (\hat{\beta}_p,s_p)$
- Assumptions: 

$$\hat{\beta}_j | \beta_j, s_j \sim N(\beta_j,s_j)$$
$$\beta_j | s_j \sim g(), \text{unimodal}.$$

- Output: estimate of $g$; "shrunken" posterior distribution for $\beta_j$.


## Generic procedure: embellishments

- Inputs: $(\hat{\beta}_1,s_1), \dots, (\hat{\beta}_p,s_p)$
- Assumptions: 

$$\hat{\beta}_j | \beta_j, s_j \sim t(\beta_j,s_j)$$
$$\beta_j | s_j \sim s_j^\alpha g(), \text{unimodal}.$$

- Output: estimate of $g$; "shrunken" posterior distribution for $\beta_j$.


## Other Applications

- Widely applicable: requiring only an estimated
effect size and standard error for each object.

- E.g. Currently applying it to wavelet shrinkage applications.


## Next steps?

- Incorporate shrinkage of variances and not just means. (e.g. ``moderated $t$ test", Smyth et al).

- Allow $g(\cdot;\pi)$ to depend on covariates $X$.

- Allow for correlations in the measured $\hat\beta_j$.

- Multivariate version: we observe a matrix of $\hat\beta_j$ and $s_j$ values.




## Thanks

- to the developers of **R**, **knitr**, **Rstudio** and **Pandoc**.
Also the very cool **SQUAREM** package.

- to the several postdoctoral researchers and students
who have worked with me on related topics.

- Including  Chaoxing Dai, Mengyin Lu, Ester Pantaleo, Scott Powers, Sen Tian, Wei Wang, Zhengrong Xing. 

- NHGRI, Moore Foundation for funding.

- `ashr` package: `http://www.github.com/stephens999/ashr`
