% Shrinkage, False Discovery Rates, and an Alternative to the Zero Assumption
% Matthew Stephens
% 2013/11/1

```{r setup, include=FALSE}
# set global chunk options
opts_chunk$set(cache=TRUE,autodep=TRUE)
dep_auto()
library("qvalue")
library("ashr")
```


# The Canonical Genomics Experiment 

- Measure lots of things, with error

- Get estimates of effects $\beta_j$ ($\hat\beta_j$) and their standard errors $s_j$

- Turn these into Z-scores, $z_j = \hat\beta_j/s_j$

- Turn these into $p$ values, $p_j$

- Apply `qvalue` 
to identify findings ``significant" at a given False Discovery Rate.

- ...?


# FDR, local fdr, and q values

Although precise definitions vary depending on whether one
takes a Bayesian or Frequentist approach to the problem, roughly

- The FDR at a threshold $P$ is 
$$\text{FDR}(P)=\Pr(\beta_j = 0 |  p_j<P).$$

- The q value for observation $j$ is $q_j=\text{FDR}(p_j)$.

- The local false discovery rate, fdr, at threshold $P$ is 
$$\text{fdr}(P) = \Pr(\beta_j =0 | p_j=P ).$$

- The fdr is more relevant, but slightly harder to estimate than 
FDR because it involves density estimation rather than tail-area estimation.

# Example: FDR estimation

```{r, echo=FALSE}
  #Set up Analysis of Hedenfalk BRCA data
  setwd("~/Documents/git/ash/paper/Rcode")

  altcol="cyan" #colors to use
  nullcol="blue" 
  nc=40 #number of bins in histograms
  ncz = 100 # number of bins in z score histograms
  
  hh = read.table("../../data/nejm_brca_release_edit.csv",sep=",",skip=3)
  subset = apply(hh, 1, max)<20
  hh = hh[subset,]

  labs = read.table("../../data/nejm_brca_release_edit.csv",sep=",",skip=1,nrows=1)
  labs = 1*(labs=="BRCA1") + 2 * (labs=="BRCA2") 

  hh.betahat = apply(hh[,labs==1],1,mean) - apply(hh[,labs==2],1,mean)
  n1 = sum(labs==1)
  n2 = sum(labs==2)
  hh.sebetahat = sqrt(apply(hh[,labs==1],1,var)/n1 + apply(hh[,labs==2],1,var)/n2)
  hh.zscore = hh.betahat/hh.sebetahat
  hh.pval = pchisq(hh.zscore^2,df=1,lower.tail=F)

```


```{r, echo=FALSE,fig.height=4,fig.cap=""}
  nc=40 #number of bins in histogram
  hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main="Distribution of p values",nclass=nc,col=altcol)

```

# Example:  FDR estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  nc=40 #number of bins in histogram
  hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main="Distribution of p values",nclass=nc,col=altcol)

  hh.q = qvalue(hh.pval)
  abline(h=hh.q$pi0,col=nullcol,lwd=2)
  
  hh.hist$density=rep(hh.q$pi0,length(hh.hist$density))  
  #hh.hist$counts=rep(hh.q$pi0*length(hh.pval)/nc,length(hh.hist$counts)) 
  plot(hh.hist,add=TRUE,col=nullcol,freq=FALSE)
```

Data from Hedenfalk et al, comparing BRCA1 vs BRCA2 expression.

# Example:  FDR estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  nc=40 #number of bins in histogram
  hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main="Distribution of p values",nclass=nc,col=altcol)

  hh.q = qvalue(hh.pval)
  abline(h=hh.q$pi0,col=nullcol,lwd=2)
  
  hh.hist$density=rep(hh.q$pi0,length(hh.hist$density))  
  #hh.hist$counts=rep(hh.q$pi0*length(hh.pval)/nc,length(hh.hist$counts)) 
  plot(hh.hist,add=TRUE,col=nullcol,freq=FALSE)
  
  abline(v=0.1,lwd=2,col=2)
  
  text(0.05,1.2,labels="A",col=2,cex=1.2)  
  text(0.05,0.4,labels="B",col=2,cex=1.2)  
  text(0.6,3,labels=paste0("FDR = B/(A+B) =  ",round(hh.q$pi0*0.1*length(hh.pval)/sum(hh.pval<0.1),2)),cex=1.2)
```



# Example: fdr estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  nc=40 #number of bins in histogram
  hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main="Distribution of p values",nclass=nc,col=altcol)

```

# Example: fdr estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  nc=40 #number of bins in histogram
  hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main="Distribution of p values",nclass=nc,col=altcol)
  
  require(fdrtool)
  hh.gren = grenander(ecdf(hh.pval))
  abline(h=min(hh.gren$f.knots),col=nullcol,lwd=2)  
  lines(hh.gren$x.knots,hh.gren$f.knots,lwd=2)
```

# Example: fdr estimation

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  nc=40 #number of bins in histogram
  hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main="Distribution of p values",nclass=nc,col=altcol)
  
  require(fdrtool)
  hh.gren = grenander(ecdf(hh.pval))
  abline(h=min(hh.gren$f.knots),col=nullcol,lwd=2)  
  lines(hh.gren$x.knots,hh.gren$f.knots,lwd=2)
  abline(v=0.1,lwd=2,col=2)
  text(0.1,0.9,labels="a",col=2,cex=1)  
  text(0.1,0.34,labels="b",col=2,cex=1.2)  
  text(0.6,3,labels=paste0("fdr = b/(a+b) =  ",round(min(hh.gren$f.knots)/approx(hh.gren$x.knots,hh.gren$f.knots,0.1)$y,2)),cex=1.2)
```

# FDR problem 1: different measurement precision

- If some effects are measured very imprecisely, those tests ``lack power"
and simply add noise

- In particular, such tests increase the estimated number of nulls, and increase
the FDR for other tests

- It would seem preferable to simply ignore the tests with very low precision. Summarizing each test by a $p$ value (or $Z$ score) loses the information about precision.

# Example: Mouse Heart Data

```{r, echo=FALSE}
setwd("~/Documents/git/ash/talks/")
## load Poisson_binomial and ash functions 
source("../../stat45800/code/Rcode/PoissonBinomial.funcs.R")  
#source("../Rcode/ash.R") 

x = read.table(paste0("../../stat45800/data/nobrega/expression/counts.txt"), header = TRUE)
xx = rowSums(x[,2:5])
x = x[xx>0,]
xx = xx[xx>0]
```

```{r echo=FALSE}
head(x)
```

- Data on 150 mouse hearts, dissected into left and right ventricle
(courtesy Scott Schmemo, Marcelo Nobrega)


```{r, echo=FALSE}

cc = x[, 2:5]

g = c(-1, -1, 1, 1)
ngene = dim(x)[1]

cc.assoc = counts.associate(cc, g, 1)
zdat.ash = cc.assoc$zdat.ash
zdat = cc.assoc$zdat

# two-sided test
ttest.pval = function(t, df) {
    pval = pt(t, df = df, lower.tail = T)
    pval = 2 * ifelse(pval < 0.5, pval, 1 - pval)
    return(pval)
}

tscore = zdat[3, ]/zdat[4, ]
pval = ttest.pval(tscore, df = 2)
qval = qvalue(pval)

highxp = xx>1000 # select high expressed genes
pval.high = pval[highxp]
qval.high = qvalue(pval.high)
cc.assoc.high = counts.associate(cc[highxp,],g,1)
zdat.ash.high = cc.assoc.high$zdat.ash
zdat.high = cc.assoc.high$zdat

```

# Example: Mouse Heart Data


```{r, echo=FALSE,fig.height=4,fig.cap=""}
h=hist(pval,prob=TRUE,ylim=c(0,4),main="p values, all genes",xlab="p value",breaks=seq(0,1,length=21),col=altcol)
abline(h=qval$pi0,col=nullcol,lwd=2)
h$density=rep(qval$pi0,length(h$density))  
plot(h,col=nullcol,add=TRUE,freq=FALSE)
abline(v=0.05,lwd=3)
text(0.6,3,labels=paste0("FDR = ",round(max(qval$qval[pval<0.05]),2)),cex=1.2)
```

# Mouse Data: Counts vary considerably across genes

```{r, echo=FALSE,fig.height=4,fig.cap=""}
hist(log10(xx),main="Distribution of total counts", xlab="log10(counts)")
```


# Lower count genes, less power

```{r, echo=FALSE,fig.height=4,fig.cap=""}
h=hist(pval[xx<1000],prob=TRUE,ylim=c(0,4),main="p values, low count genes",xlab="p value",breaks=seq(0,1,length=21),col=altcol)
qval.low=qvalue(pval[xx<1000])
pval.low=pval[xx<1000]
abline(h=qval.low$pi0,col=nullcol,lwd=2)
h$density=rep(qval.low$pi0,length(h$density))  
plot(h,col=nullcol,add=TRUE,freq=FALSE)
abline(v=0.05,lwd=3)
#text(0.6,3,labels=paste0("FDR = ",round(max(qval.low$qval[pval.low<0.05]),2)),cex=1.2)
```


# Higher count genes, more power
```{r, echo=FALSE, fig.height=4,fig.cap=""}

h=hist(pval.high,prob=TRUE,ylim=c(0,4),main="p values, high count genes",xlab="p value",col=altcol)

abline(h=qval.high$pi0,col=nullcol,lwd=2)
h$density=rep(qval.high$pi0,length(h$density))  
plot(h,col=nullcol,add=TRUE,freq=FALSE)
abline(v=0.05,lwd=3)
#text(0.6,3,labels=paste0("FDR = ",round(max(qval.high$qval[pval.high<0.05]),2)),cex=1.2)
#qval.high$q[pval.high<0.05] gives FDR for p<0.05
```

# FDR problem 1: low count genes add noise, increase q values
```{r,echo=FALSE, fig.height=4,fig.cap=""}
plot(qval.high$qval,qval$qval[xx>1000],xlab="q values from high-count gene analysis", ylab="q values from all gene analysis", main="q values for high count genes",xlim=c(0,.5),ylim=c(0,.5))
abline(a=0,b=1,col=2)
```

# FDR problem 1: Summary

- Analyzing $p$ values or $Z$ scores doesn't fully account 
for measurement precision.

# Problem 2: The Zero Assumption (ZA)

- The standard `qvalue` 
approach assumes that all the $p$ values near 1 are null.

- Analogously, one can assume that all Z scores near 0 are null. Efron refers to this as the ``Zero Assumption".

- The ZA allows us to estimate the null proportion, $\pi_0$, using the density of $p$ values near 1 (or $Z$ scores near 0).


# Problem 2: The ZA 

- The ZA seems initially natural.

- However, it turns out to imply unrealistic assumptions about the distribution of non-zero effects.



```{r, echo=FALSE, include=FALSE}
require(fdrtool)
  hh.fdrtool = fdrtool(hh.pval,statistic="pvalue",plot=FALSE)
require(locfdr)
  hh.locfdr = locfdr(hh.zscore,nulltype=0,plot=0)
require(mixfdr)
  hh.mixfdr = mixFdr(hh.zscore,noiseSD=1,theonull=TRUE,plot=FALSE)
```

# Implied distribution of $p$ values under $H_1$

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  nc=40 #number of bins in histogram
  hh.hist=hist(hh.pval,freq=FALSE,xlab="p value",main="Distribution of p values",nclass=nc,col=altcol)

  hh.q = qvalue(hh.pval)
  abline(h=hh.q$pi0,col=nullcol,lwd=2)
  
  hh.hist$density=rep(hh.q$pi0,length(hh.hist$density))  
  #hh.hist$counts=rep(hh.q$pi0*length(hh.pval)/nc,length(hh.hist$counts)) 
  plot(hh.hist,add=TRUE,col=nullcol,freq=FALSE)
  
  abline(v=0.1,lwd=2,col=2)
  
  text(0.05,1.2,labels="A",col=2,cex=1.2)  
  text(0.05,0.4,labels="B",col=2,cex=1.2)  
  text(0.6,3,labels=paste0("FDR = B/(A+B) =  ",round(hh.q$pi0*0.1*length(hh.pval)/sum(hh.pval<0.1),2)),cex=1.2)
```


# Implied distribution of Z scores under alternative (fdrtool)

```{r, echo=FALSE,fig.height=4,fig.cap=""}
#plot a histogram of z scores, highlighting the alternative distribution
#of z scores that is implied by localfdr values lfdr.
  nullalthist = function(z,lfdr,...){
    h=hist(z, freq=FALSE,col=nullcol,nclass=ncz,...)
    avlfdr = unlist(lapply(split(lfdr,cut(z,h$breaks),drop=FALSE),mean))
    h$density = (1-avlfdr) * h$density
    plot(h,add=TRUE,col=altcol,freq=FALSE)
  }
  
 
  
  nullalthist(hh.zscore,hh.fdrtool$lfdr,main="fdrtool")  
```

# Implied distribution of Z scores under alternative (locfdr)

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  
  nullalthist(hh.zscore,hh.locfdr$fdr,main="locfdr")
```

# Implied distribution of Z scores under alternative (mixfdr)

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  
  nullalthist(hh.zscore,hh.mixfdr$fdr,main="mixfdr")
  dev.off()
```

# Problems: Summary

- By summarizing each observation by a $Z$ score or $p$ value, 
standard fdr tools ignore precision of different measurements

- Standard tools make the ZA, which implies actual effects have a (probably unrealistic) bimodal distribution. [and tends to overestimate $\pi_0$, losing power]

- Also standard tools focus only on zero vs non-zero effects. (eg what if we would
like to identify genes that have at least a 2-fold change?)

# FDR via Empirical Bayes

- Following previous work (e.g. Newton, Efron, Muralidharan) we take an empirical Bayes approach to FDR.

- Eg Efron assumes that the $Z$ scores come from a mixture of null, and alternative:
$$Z_j \sim f_Z(.) = \pi_0 N(.;0,1) + (1-\pi_0) f_1(.)$$
where $f_1$ is to be estimated from the data.

- Various semi-parametric approaches taken to estimating $f_1$. For example,
Efron uses Poisson regression; Muralidharan uses mixture of normal distributions.

- $\text{fdr}(Z) \approx \pi_0 N(Z; 0,1)/ f_Z(Z)$

# FDR: The New Deal

- Instead of modelling $Z$ scores, model the effects $\beta$,
$$\beta_j \sim \pi_0 \delta_0(.) + (1-\pi_0) g(.)$$

- Constrain $g$ to be unimodal about 0.

- *Incorporate precision* of each observation $\hat\beta$ into the likelihood.
Specifically, approximate likelihood for $\beta_j$ by a normal: 
$$L(\beta_j) \propto \exp(-0.5 (\beta_j - \hat\beta_j)^2/s_j^2).$$
[From $\hat\beta_j \sim N(\beta_j, s_j)$]

- fdr given by $$p(\beta_j =0 | \hat\beta_j) = \pi_0 p(\hat\beta_j | \beta_j=0)/p(\hat\beta_j)$$


# FDR - The New Deal

- A convenient way to model $g$ is by a mixture of 0-centered
normal distributions: 
$$g(\beta; \pi) = \sum_{k=1}^K \pi_k N(\beta; 0, \sigma^2_k)$$

- Estimating $g$ comes down to estimating $\pi$. Joint estimation of $\pi_0,\pi$ easy by maximum likelihood (EM algorithm) or variational Bayes.

- By allowing $K$ large, and $\sigma_k$ to span a dense grid of values,
we get a fairly flexible unimodal symmetric distribution.

- Alternatively, a mixture of uniforms, with 0 as one end-point of the range,
provides still more flexibility, and in particular allows for asymmetry. 

- If allow a very large number of uniforms this provides the non-parametric mle for $g$; cf Grenander 1953; Campy + Thomas.


# Illustration: $g$ a mixture of 0-centered normals

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, dnorm(x,0,1),type="l",ylim=c(0,2),ylab="density")
lines(x, dnorm(x,0,0.1))
lines(x, dnorm(x,0,0.5))
```

# Illustration: $g$ a mixture of 0-centered normals

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, 0.5*dnorm(x,0,1)+0.5*dnorm(x,0,0.1),type="l",ylim=c(0,2),ylab="density")
```


# Illustration: $g$ a mixture of 0-anchored uniforms

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, dunif(x,0,1),type="s",ylim=c(0,5),ylab="density")
lines(x, dunif(x,0,0.2),type="s")
lines(x, dunif(x,0,0.5),type="s")
lines(x, dunif(x,-0.3,0),type="s",col=2)
lines(x, dunif(x,-0.4,0),type="s",col=2)
lines(x, dunif(x,-2,0),type="s",col=2)
```

# Illustration: $g$ a mixture of 0-anchored uniforms

```{r, echo=FALSE,fig.height=4,fig.cap=""}
x=seq(-4,4,length=100)
plot(x, 0.1*dunif(x,0,3)+ 0.3*dunif(x,0,0.2) + 0.2*dunif(x,0,0.5) + 0.1* dunif(x,-0.3,0) + 0.1*dunif(x,-0.3,0)+0.2*dunif(x,-0.4,0),type="s",ylim=c(0,2),ylab="density")
```

# Issue: identifiability of $\pi_0$

- For estimating False Discoveries, we are asking whether $\beta_j = 0$.

- However, the data cannot distinguish between $\beta_j = 0$ and $\beta_j$ "very
small"

- As a result $\pi_0$ is formally unidentifiable. Eg data can never rule out $\pi_0=0$.


# Issue: identifiability of $\pi_0$

- The Zero assumption (ZA) solves the identifiability problem by assuming that
there *are* no $\beta_j$ near zero!

- The ZA makes $\pi_0$ identifiable. 

- Another view is that the estimate of $\pi_0$ under ZA will systematically tend to overestimate $\pi_0$, and so is ``conservative".

- That is it provides an ``upper bound" on $\pi_0$

# Identifiability of $\pi_0$: Solution 1

- We replaced the ZA with the unimodal assumption on g.

- This does not make $\pi_0$ identifiable, but it does effectively provide an upper bound on $\pi_0$. 

- Indeed, we saw that when we estimated $\pi_0$ under the ZA the data
then contradicted the unimodal assumption on g. Thus the upper bound is
more conservative than under ZA.

- In practice, implement upper bound by putting prior on $\pi_0$ that encourages it to be big, then estimate $\pi$ by posterior mean (VB).

# Illustration: BRCA data

```{r, echo=FALSE,include=FALSE}
hh.ash = ash(hh.betahat,hh.sebetahat)
hh.ashz = ash(hh.zscore,1, pointmass=TRUE, prior="nullbiased")
hh.ash.hu = ash(hh.betahat, hh.sebetahat,mixcompdist="halfuniform")
```


# Example: BRCA data

Compare fitted $f(\beta)$, both estimating $\pi_0$ and fixing $\pi_0=0$.
```{r, echo=FALSE, includ=FALSE}
  hh.ash.fdr = ash(hh.zscore,1,pointmass = TRUE,prior="nullbiased")
  hh.ash.shrink = ash(hh.zscore,1)
```

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  plot(x,cdf.ash(hh.ash.fdr,x),col=2,type="l",lwd=3,ylab="cdf")
  lines(x,cdf.ash(hh.ash.shrink,x),col=3,lwd=3,lty=2)
```

# Recall Problem: distribution of alternative Z values multimodal
```{r, echo=FALSE,fig.height=4,fig.cap=""}
nullalthist(hh.zscore,hh.mixfdr$fdr,main="mixfdr")
```

# Problem Fixed: distribution of alternative Z values unimodal
```{r, echo=FALSE,fig.height=4,fig.cap=""}
nullalthist(hh.zscore,hh.ashz$ZeroProb,main="ash")
```


# BRCA1: Compare $\pi_0$ estimates
```{r}
  round(c(hh.fdrtool$param[3],
  hh.locfdr$fp0[1,3],
  hh.mixfdr$pi[1],
  hh.ashz$fitted.g$pi[1]),2)
```

# BRCA1: Compare number significant at fdr<0.05

```{r}
  c(sum(hh.fdrtool$lfdr<0.05),
  sum(hh.locfdr$fdr<0.05),
  sum(hh.mixfdr$fdr<0.05),
  sum(hh.ashz$ZeroProb<0.05))
```


# Identifiability of $\pi_0$: Solution 2

- Identifiability of $\pi_0$ is primarily an issue if we insist on asking question is $\beta_j=0$?

- How about we change focus: assume *none* of the $\beta_j$ are zero ("one group approach"), and ask for which $\beta_j$ are we confident about the sign (Gelman et al, 2012).

- Positive and negative effects are often treated differently in practice anyway.

- That is we replace fdr with False Sign Rate (fsr), the probability that if we say an effect is positive (negative), it is not.

- Example: suppose we estimate that $\Pr(\beta_j<0)=0.975$ and $\Pr(\beta_j>0)=0.025$. Then we report $\beta_j$ as a ``(negative) discovery", and estimate its fsr as 0.025.

# A technicality

- Suppose you estimate $\Pr(\beta_j<0)=0.98$,  $\Pr(\beta_j>0)=0.01$, $\Pr(\beta_j=0) = 0.01$. 

- Should you declare an fdr of 0.01 or 0.02?

- Maybe fsr makes more sense anyway?


# Unification of one-group and two-group solutions

- A possible concern with one-group model: if we assume all $\beta$ are non-zero, but really some are actually zero, might we not underestimate the fsr?

- Consider our example, with $\Pr(\beta_j>0)=0.025$. If we actually allowed
$\beta_j=0$ then possibly all of this probability might actually land at $\beta_j=0$. 

- I argue that, assuming symmetry of $g$ near 0, this also provides an upper
bound of how much of the $\Pr(\beta_j<0)=0.975$ might also move to 0.

- Therefore a more conservative estimate of the fsr might be 0.05 (or, more generally, double what you get allowing for point mass)


# Example: BRCA data

```{r, echo=FALSE,fig.height=4,fig.cap=""}
  plot(hh.ash.fdr$localfsr,2*hh.ash.shrink$localfsr,xlab="fsr with point mass", ylab="2 * fsr without point mass", xlim=c(0,0.1),ylim=c(0,0.1))
  abline(a=0,b=1,col=2,lwd=2)
```

# Estimation and Shrinkage

- Besides allowing one to estimate fdr and fsr, 
this approach also provides a full posterior distribution for each $\beta_j$. 

- So for example we can easily compute fdrs for discoveries other than ``non-zero" (eg compute $\Pr(|\beta_j| > 2 \| \hat\beta_j)$).

- And use it to obtain point estimates and credible intervals for each $\beta_j$, taking account of information from all the other $\beta_j$.

- Because $f(\beta)$ is unimodal, the point estimates will tend to be ``shrunk" towards the overall mean (0).

- Because $f(\beta)$ is estimated from the data, the amount
of shrinkage is adaptive to the data. And because of the role of $s_j$, the amount of shrinkage adapts to the information on each gene.

- So we call the approach ``Adaptive Shrinkage" (ASH).


# Example: ASH applied to mouse data

```{r, fig.height=4,echo=FALSE,fig.cap=""}
hist(zdat[3,],main="Raw effect size estimates",xlab="betahat",xlim=c(-2,2),prob=T,ylim=c(0,3))
```

# Example: ASH applied to mouse data

```{r, fig.height=4,echo=FALSE,fig.cap=""}
hist(zdat[3,],main="Raw effect size estimates",xlab="betahat",xlim=c(-2,2),prob=T,ylim=c(0,3))
t=seq(-2,2,length=201)
lines(t,dnorm(t,0,sd=0.032), col=2, lwd=2)
```

# Example: ASH applied to mouse data

```{r, fig.height=4,echo=FALSE,fig.cap=""}
hist(zdat.ash$PosteriorMean,main="Shrunken estimates",xlab="shrunk betahat",xlim=c(-0.1,0.1),prob=T)
lines(t,dnorm(t,0,sd=0.032), col=2, lwd=2)
```


# Shrinkage is adaptive to information

```{r, echo=FALSE,fig.height=4}
#hist(zdat[3,])
#plot(zdat[3,],zdat.ash$PosteriorMean,xlim=c(-0.2,0.2))
#points(zdat[3,16677],zdat.ash$PosteriorMean[16677],col=2,pch=16)
#points(zdat[3,16079],zdat.ash$PosteriorMean[16079],col=2,pch=16)
#x[16677,]
#x[16079,]
```


```{r, echo=FALSE}
#temp = counts.associate(cc,c(-1,1,-1,1),1)
#temp.tscore = temp$zdat[3,]/temp$zdat[4,]
#temp.pval = ttest.pval(temp.tscore,df=2)
#plot(temp$zdat.ash$localfdr,temp.pval)
#identify(temp$zdat.ash$localfdr,temp.pval)
```

```{r, echo=FALSE,fig.height=3,fig.cap=""}
plot(zdat.ash$localfdr,pval,ylab="p value", xlab="ASH local fdr")
```

# Shrinkage is adaptive to information

```{r, echo=FALSE,fig.height=3,fig.cap=""}
plot(zdat.ash$localfdr,pval,ylab="p value", xlab="ASH local fdr")
points(zdat.ash$localfdr[15325],pval[15325],col=2,pch=16)
points(zdat.ash$localfdr[16123],pval[16123],col=2,pch=16)
```

# Shrinkage is adaptive to information

```{r, echo=FALSE}
x = read.table(paste0("../../stat45800/data/nobrega/expression/counts.txt"), header = TRUE)
xx = rowSums(x[,2:5])
x = x[xx>0,]
xx = xx[xx>0]
cbind(x[,1:5],pval,zdat.ash$localfdr)[c(15325,16123),]
```

# Recall FDR problem 1: q values increased by low count genes
```{r,echo=FALSE, fig.height=4,fig.cap=""}
plot(qval.high$qval,qval$qval[xx>1000],xlab="q values from high-count gene analysis", ylab="q values from all gene analysis", main="q values for high count genes",xlim=c(0,.5),ylim=c(0,.5))
abline(a=0,b=1,col=2)
```

# ASH q values more robust to inclusion of low count genes

```{r, echo=FALSE,fig.height=3,fig.cap=""}
plot(zdat.ash.high$qvalue,zdat.ash$qvalue[xx>1000],xlab="q values from high only",ylab="q values from all data")
abline(a=0,b=1,col=2,lwd=2)
```


# Summary: FDR via Adapative Shrinkage

- Both provide a rational approach to identifying ``significant" findings.

- Both are generic and modular: once you have the summary data, you can forget where they came from.  

- But by using two numbers ($\hat\beta,s$) instead of one ($p$ values) precision of different measurementscan be better accounted for.

- ASH borrows information for estimation, as well as testing.

# Other Applications

- Widely applicable: perhaps anywhere (?) 
where shrinkage is appropriate, requiring only an estimated
effect size and standard error for each object.

- Could also use effect size estimate and $p$ value for each variable, by converting to effect size estimate and (pseudo-) standard error.

- Currently applying it to wavelet shrinkage applications.


# Guarantees?

- ``I think you have some nice ideas. How will you convince
people to use them?" (C Morris)

# Next steps?

- Extend to allow $g(\cdot;\pi)$ to depend on covariates $X$.

- Extend to allow for correlations in the measured $\hat\beta_j$.


# Thanks

- to the several postdoctoral researchers and students
who have worked with me on related topics.

- Especially Mengyin Lu who coded the VB algorithm.


# Reproducible research

- This document is produced with **knitr**, **Rstudio** and **Pandoc**.

- For more details see my `stephens999/ash` repository at `http://www.github.com/stephens999/ash`

- Website: `http://stephenslab.uchicago.edu`

# Pandoc Command used

`pandoc -s -S -i --template=my.beamer -t beamer -V theme:CambridgeUS -V colortheme:beaver  ilike-slides.md -o ilike-slides.pdf`

(alternative to produce html slides; but figures would need reworking)
`pandoc -s -S -i -t dzslides --mathjax NSmeet2013.md -o NSmeet2013.html`

Here is my session info:

```{r session-info}
print(sessionInfo(), locale=FALSE)
```




# Some odd things in the data

```{r, echo=FALSE,fig.height=3}
  plot(zdat[3,],zdat[4,],ylab="standard error",xlab="beta-hat")
  x[tail(order(zdat[3,])),]
```
